{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "torch.manual_seed(265)\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "          else torch.device('cpu'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def load_cifar(train_val_split=0.9, data_path='../data/', preprocessor=None):\n",
    "    \n",
    "    # Define preprocessor if not already given\n",
    "    if preprocessor is None:\n",
    "        preprocessor = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                                (0.2470, 0.2435, 0.2616))\n",
    "        ])\n",
    "    \n",
    "    # load datasets\n",
    "    data_train_val = datasets.CIFAR10(\n",
    "        data_path,       \n",
    "        train=True,      \n",
    "        download=True,\n",
    "        transform=preprocessor)\n",
    "\n",
    "    data_test = datasets.CIFAR10(\n",
    "        data_path, \n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=preprocessor)\n",
    "\n",
    "    # train/validation split\n",
    "    n_train = int(len(data_train_val)*train_val_split)\n",
    "    n_val =  len(data_train_val) - n_train\n",
    "\n",
    "    data_train, data_val = random_split(\n",
    "        data_train_val, \n",
    "        [n_train, n_val],\n",
    "        generator=torch.Generator().manual_seed(123)\n",
    "    )\n",
    "    \n",
    "    return (data_train, data_val, data_test)\n",
    "\n",
    "# Loading entire dataset \n",
    "cifar10_train, cifar10_val, cifar10_test = load_cifar()\n",
    "\n",
    "# Defining the two labels: plane, bird:\n",
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "\n",
    "# For each dataset, keep only plane and birds\n",
    "cifar2_train = [(img, label_map[label]) for img, label in cifar10_train if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label]) for img, label in cifar10_val if label in [0, 2]]\n",
    "cifar2_test = [(img, label_map[label]) for img, label in cifar10_test if label in [0, 2]]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 4504, 0: 4513})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([label for _, label in cifar2_train])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP(nn.Module): \n",
    "\n",
    "    def __init__(self, in_dim, out_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.dimension = in_dim\n",
    "        self.fc1 = nn.Linear(in_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.af = nn.ReLU()\n",
    "\n",
    "    def forward(self, x): \n",
    "        out = torch.flatten(x, 1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = F.relu(self.fc3(out))\n",
    "        return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    \n",
    "    n_batch = len(train_loader)\n",
    "    losses_train = []\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        \n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "\n",
    "            imgs = imgs.to(device=device, dtype=torch.double) \n",
    "            labels = labels.to(device=device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        losses_train.append(loss_train / n_batch)\n",
    "\n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.3f}'.format(\n",
    "                datetime.now().time(), epoch, loss_train / n_batch))\n",
    "    return losses_train\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3.1.4, 3.1.6, 3.1.7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_manual_update(n_epochs, lr, model, loss_fn, train_loader, weight_decay = 0, momentum = 0):\n",
    "    \n",
    "    n_batch = len(train_loader)\n",
    "    losses_train = []\n",
    "    model.train()\n",
    "    \n",
    "    t = 0\n",
    "    b = []\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        \n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "\n",
    "            imgs = imgs.to(device=device, dtype=torch.double) \n",
    "            labels = labels.to(device=device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    " \n",
    "            index = 0\n",
    "\n",
    "            for p in model.parameters(): \n",
    "\n",
    "                g = p.grad \n",
    "\n",
    "                # L2 regularization 0 < weight_decay < 1, \n",
    "                # 0 if not  L2 regularization\n",
    "                g += weight_decay*p.data\n",
    "\n",
    "                # Momentum          0 < momentum < 1, \n",
    "                # 0 if not momentum\n",
    "\n",
    "                if t == 0: \n",
    "                    b.append(g)\n",
    "                else: \n",
    "                    b[index] = momentum*b[index] + g\n",
    "\n",
    "                g = b[index]\n",
    "                p.data = p.data - lr*g             #Formel 3 \n",
    "                p.grad = torch.zeros_like(p.grad)\n",
    "                index += 1\n",
    "                \n",
    "            t += 1\n",
    "\n",
    "            \n",
    "            \n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        losses_train.append(loss_train / n_batch)\n",
    "\n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.3f}'.format(\n",
    "                datetime.now().time(), epoch, loss_train / n_batch))\n",
    "    return losses_train\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3.1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:37:30.143786  |  Epoch 1  |  Training loss 1.117\n",
      "14:37:37.456390  |  Epoch 5  |  Training loss 0.748\n",
      "14:37:47.138513  |  Epoch 10  |  Training loss 0.746\n",
      "14:37:56.018489  |  Epoch 15  |  Training loss 0.745\n",
      "14:38:05.297155  |  Epoch 20  |  Training loss 0.745\n",
      " \n",
      "14:38:09.592313  |  Epoch 1  |  Training loss 1.117\n",
      "14:38:20.093011  |  Epoch 5  |  Training loss 0.748\n",
      "14:38:33.653460  |  Epoch 10  |  Training loss 0.746\n",
      "14:38:46.762490  |  Epoch 15  |  Training loss 0.745\n",
      "14:38:59.057699  |  Epoch 20  |  Training loss 0.745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1167964031696134,\n",
       " 0.7556445065022518,\n",
       " 0.7511422467673443,\n",
       " 0.7490721194427126,\n",
       " 0.747860450199053,\n",
       " 0.7470661515259628,\n",
       " 0.7465170398577246,\n",
       " 0.7461190914341219,\n",
       " 0.7458358342895215,\n",
       " 0.7456563293877976,\n",
       " 0.7454918606574988,\n",
       " 0.7454121516793184,\n",
       " 0.7453679508661936,\n",
       " 0.745338394823468,\n",
       " 0.7453206327056063,\n",
       " 0.7452971163841055,\n",
       " 0.7452781646487129,\n",
       " 0.7452469508327764,\n",
       " 0.74522061382151,\n",
       " 0.745207550636384,\n",
       " 0.7452008092028434]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cifar2_train \n",
    "# cifar2_val \n",
    "# cifar2_test \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2_train, batch_size=64, shuffle=False) \n",
    "\n",
    "# Instantiate the loss function (here we use cross entropy)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model_train = MyMLP(3072, 2)\n",
    "optimizer = optim.SGD(model_train.parameters(), lr=1e-2, weight_decay=0.5, momentum=0.5)\n",
    "\n",
    "\n",
    "# Now all we have to do is calling the training loop\n",
    "# WARNING THIS MIGHT BE EXTREMELY SLOW. STOP YOUR KERNEL TO STOP THE TRAINING\n",
    "train(\n",
    "    n_epochs = 21,\n",
    "    optimizer = optimizer,\n",
    "    model = model_train,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "print(' ')\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model_train_manual_update = MyMLP(3072, 2)\n",
    "\n",
    "train_manual_update(\n",
    "    n_epochs = 21,\n",
    "    lr = 1e-2,\n",
    "    model = model_train_manual_update,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    weight_decay=0.5,\n",
    "    momentum=0.5\n",
    ")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += labels.shape[0]\n",
    "            correct += int((predicted == labels).sum())\n",
    "\n",
    "    acc =  correct / total\n",
    "    print(\"Accuracy: {:.2f}\".format(acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:39:04.222012  |  Epoch 1  |  Training loss 1.416\n",
      "14:39:14.009795  |  Epoch 5  |  Training loss 0.486\n",
      "14:39:26.177106  |  Epoch 10  |  Training loss 0.449\n",
      "14:39:39.580322  |  Epoch 15  |  Training loss 0.430\n",
      "14:39:53.298103  |  Epoch 20  |  Training loss 0.418\n",
      "MODEL1\n",
      "Training accuracy:\n",
      "Accuracy: 0.85\n",
      "Validation accuracy:\n",
      "Accuracy: 0.83\n",
      "14:39:59.359519  |  Epoch 1  |  Training loss 1.093\n",
      "14:40:10.268266  |  Epoch 5  |  Training loss 0.474\n",
      "14:40:23.626039  |  Epoch 10  |  Training loss 0.444\n",
      "14:40:37.722405  |  Epoch 15  |  Training loss 0.429\n",
      "14:40:51.523850  |  Epoch 20  |  Training loss 0.420\n",
      "MODEL2\n",
      "Training accuracy:\n",
      "Accuracy: 0.85\n",
      "Validation accuracy:\n",
      "Accuracy: 0.82\n",
      "14:40:57.432339  |  Epoch 1  |  Training loss 0.873\n",
      "14:41:08.919387  |  Epoch 5  |  Training loss 0.465\n",
      "14:41:22.192279  |  Epoch 10  |  Training loss 0.439\n",
      "14:41:36.046806  |  Epoch 15  |  Training loss 0.428\n",
      "14:41:51.277644  |  Epoch 20  |  Training loss 0.422\n",
      "MODEL3\n",
      "Training accuracy:\n",
      "Accuracy: 0.85\n",
      "Validation accuracy:\n",
      "Accuracy: 0.82\n",
      "14:41:57.966818  |  Epoch 1  |  Training loss 3.113\n",
      "14:42:09.802690  |  Epoch 5  |  Training loss 2.264\n",
      "14:42:25.127078  |  Epoch 10  |  Training loss 0.558\n",
      "14:42:40.429658  |  Epoch 15  |  Training loss 0.481\n",
      "14:42:55.628965  |  Epoch 20  |  Training loss 0.459\n",
      "MODEL4\n",
      "Training accuracy:\n",
      "Accuracy: 0.83\n",
      "Validation accuracy:\n",
      "Accuracy: 0.82\n",
      "14:43:02.206884  |  Epoch 1  |  Training loss 1.234\n",
      "14:43:14.287484  |  Epoch 5  |  Training loss 0.483\n",
      "14:43:28.046803  |  Epoch 10  |  Training loss 0.449\n",
      "14:43:40.738987  |  Epoch 15  |  Training loss 0.433\n",
      "14:43:53.292213  |  Epoch 20  |  Training loss 0.423\n",
      "MODEL5\n",
      "Training accuracy:\n",
      "Accuracy: 0.85\n",
      "Validation accuracy:\n",
      "Accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "best_model = None \n",
    "best_acc = 0\n",
    "best_lr = None \n",
    "best_weight_decay = None \n",
    "best_momentum = None \n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_loader = torch.utils.data.DataLoader(cifar2_train, batch_size=64, shuffle=False) \n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "model1 = MyMLP(3072, 2)\n",
    "model2 = MyMLP(3072, 2)\n",
    "model3 = MyMLP(3072, 2)\n",
    "model4 = MyMLP(3072, 2)\n",
    "model5 = MyMLP(3072, 2)\n",
    "\n",
    "\n",
    "train_manual_update(n_epochs = 21, lr = 0.001, model = model1, loss_fn = loss_fn, train_loader = train_loader, weight_decay=0.1, momentum=0.9)\n",
    "print(f'MODEL1')\n",
    "print(\"Training accuracy:\")\n",
    "train_acc_1 = compute_accuracy(model1, train_loader)\n",
    "print(\"Validation accuracy:\")\n",
    "val_acc_1 = compute_accuracy(model1, val_loader)\n",
    "\n",
    "if best_acc < val_acc_1: \n",
    "    best_model = model1 \n",
    "    best_acc = val_acc_1\n",
    "    best_lr =  0.0010\n",
    "    best_weight_decay =  0.1\n",
    "    best_momentum =  0.9\n",
    "\n",
    "\n",
    "\n",
    "train_manual_update(n_epochs = 21, lr = 0.01, model = model2, loss_fn = loss_fn, train_loader = train_loader, weight_decay=0.1, momentum=0.25)\n",
    "print(f'MODEL2')\n",
    "print(\"Training accuracy:\")\n",
    "train_acc_2 = compute_accuracy(model2, train_loader)\n",
    "print(\"Validation accuracy:\")\n",
    "val_acc_2 = compute_accuracy(model2, val_loader)\n",
    "\n",
    "if best_acc < val_acc_2: \n",
    "    best_model = model2\n",
    "    best_acc = val_acc_2\n",
    "    best_lr = 0.01 \n",
    "    best_weight_decay = 0.1\n",
    "    best_momentum = 0.25 \n",
    "\n",
    "\n",
    "\n",
    "train_manual_update(n_epochs = 21, lr = 0.01, model = model3, loss_fn = loss_fn, train_loader = train_loader, weight_decay=0.1, momentum=0.5)\n",
    "print(f'MODEL3')\n",
    "print(\"Training accuracy:\")\n",
    "train_acc_3 = compute_accuracy(model3, train_loader)\n",
    "print(\"Validation accuracy:\")\n",
    "val_acc_3 = compute_accuracy(model3, val_loader)\n",
    "\n",
    "if best_acc < val_acc_3: \n",
    "    best_model = model3 \n",
    "    best_acc = val_acc_3\n",
    "    best_lr = 0.01 \n",
    "    best_weight_decay = 0.1\n",
    "    best_momentum = 0.5 \n",
    "\n",
    "\n",
    "\n",
    "train_manual_update(n_epochs = 21, lr = 0.001, model = model4, loss_fn = loss_fn, train_loader = train_loader, weight_decay=0.1, momentum=0.75)\n",
    "print(f'MODEL4')\n",
    "print(\"Training accuracy:\")\n",
    "train_acc_4 = compute_accuracy(model4, train_loader)\n",
    "print(\"Validation accuracy:\")\n",
    "val_acc_4 = compute_accuracy(model4, val_loader)\n",
    "\n",
    "if best_acc < val_acc_4: \n",
    "    best_model = model4\n",
    "    best_acc = val_acc_4\n",
    "    best_lr = 0.001 \n",
    "    best_weight_decay = 0.1 \n",
    "    best_momentum = 0.75 \n",
    "\n",
    "\n",
    "\n",
    "train_manual_update(n_epochs = 21, lr = 0.01, model = model5, loss_fn = loss_fn, train_loader = train_loader, weight_decay=0.1, momentum=0.1)\n",
    "print(f'MODEL5')\n",
    "print(\"Training accuracy:\")\n",
    "train_acc_5 = compute_accuracy(model5, train_loader)\n",
    "print(\"Validation accuracy:\")\n",
    "val_acc_5 = compute_accuracy(model5, val_loader)\n",
    "\n",
    "if best_acc < val_acc_5: \n",
    "    best_model = model5 \n",
    "    best_acc = val_acc_5\n",
    "    best_lr = 0.01 \n",
    "    best_weight_decay = 0.1 \n",
    "    best_momentum = 0.1 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyMLP(\n",
      "  (fc1): Linear(in_features=3072, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (af): ReLU()\n",
      ")\n",
      "0.8260427263479145\n",
      "0.001\n",
      "0.1\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "print(best_model )\n",
    "print(best_acc)\n",
    "print(best_lr )\n",
    "print(best_weight_decay )\n",
    "print(best_momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.83\n",
      "0.8260427263479145\n"
     ]
    }
   ],
   "source": [
    "test_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "test_acc = compute_accuracy(best_model, test_loader)\n",
    "\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLeNet5(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__() \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=2)  \n",
    "        self.norm1 = nn.BatchNorm2d(6)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.norm2 = nn.BatchNorm2d(16)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.flat = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(in_features=9*12*16, out_features=120)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = torch.relu(self.pool1(out))\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out)\n",
    "        out = torch.relu(self.pool2(out))\n",
    "        \n",
    "        out = self.flat(out)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = torch.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "\n",
    "        return out\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inf265",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b15b97dbde57fe5dbb83da9804c15430f96a82d4a50a5d957c01b9b24a43bde"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
